{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f89b95",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f565c",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7865d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53de7510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b2087",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d7b96",
   "metadata": {},
   "source": [
    "### Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f899b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\"C:/Users/sesha/Desktop/Projects/CNN - Car or bike/dataset/training_set\",\n",
    "                                                target_size=(64,64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c584d",
   "metadata": {},
   "source": [
    "### Preprocessing the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b928f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('C:/Users/sesha/Desktop/Projects/CNN - Car or bike/dataset/test_set',\n",
    "                                           target_size=(64,64),\n",
    "                                           batch_size=32,\n",
    "                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b342ca",
   "metadata": {},
   "source": [
    "## 2. Constructing the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70441d",
   "metadata": {},
   "source": [
    "### Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66ca0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0211b63a",
   "metadata": {},
   "source": [
    "### 1. Convolusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba08abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=40, kernel_size=3, activation='relu', input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37f50d",
   "metadata": {},
   "source": [
    "### 2. Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc0da416",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0040f6",
   "metadata": {},
   "source": [
    "### Adding a 2nd Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c81b5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=40, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32da41",
   "metadata": {},
   "source": [
    "### 3. Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b1c06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e874da",
   "metadata": {},
   "source": [
    "### 4. Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "122fd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb65853",
   "metadata": {},
   "source": [
    "### 5. Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5b52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77eb099",
   "metadata": {},
   "source": [
    "## 3. Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddded946",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090ef1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be77501",
   "metadata": {},
   "source": [
    "### Training the CNN on the Training Dataset and Evaluting the Data on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "943ed91f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " 20/113 [====>.........................] - ETA: 28s - loss: 0.7109 - acc: 0.5484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 230ms/step - loss: 0.5214 - acc: 0.7306 - val_loss: 0.4944 - val_acc: 0.7850\n",
      "Epoch 2/60\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.3045 - acc: 0.8689 - val_loss: 0.3681 - val_acc: 0.8625\n",
      "Epoch 3/60\n",
      "113/113 [==============================] - 24s 213ms/step - loss: 0.2451 - acc: 0.8964 - val_loss: 0.4088 - val_acc: 0.8575\n",
      "Epoch 4/60\n",
      "113/113 [==============================] - 25s 224ms/step - loss: 0.2260 - acc: 0.9069 - val_loss: 0.3485 - val_acc: 0.8925\n",
      "Epoch 5/60\n",
      "113/113 [==============================] - 27s 242ms/step - loss: 0.2160 - acc: 0.9144 - val_loss: 0.5235 - val_acc: 0.7850\n",
      "Epoch 6/60\n",
      "113/113 [==============================] - 25s 222ms/step - loss: 0.1892 - acc: 0.9233 - val_loss: 0.4967 - val_acc: 0.8350\n",
      "Epoch 7/60\n",
      "113/113 [==============================] - 25s 225ms/step - loss: 0.1872 - acc: 0.9272 - val_loss: 0.3177 - val_acc: 0.8925\n",
      "Epoch 8/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.1612 - acc: 0.9394 - val_loss: 0.4512 - val_acc: 0.8475\n",
      "Epoch 9/60\n",
      "113/113 [==============================] - 25s 219ms/step - loss: 0.1567 - acc: 0.9353 - val_loss: 0.4405 - val_acc: 0.8575\n",
      "Epoch 10/60\n",
      "113/113 [==============================] - 25s 218ms/step - loss: 0.1547 - acc: 0.9392 - val_loss: 0.3491 - val_acc: 0.8850\n",
      "Epoch 11/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.1380 - acc: 0.9478 - val_loss: 0.3876 - val_acc: 0.8700\n",
      "Epoch 12/60\n",
      "113/113 [==============================] - 25s 222ms/step - loss: 0.1400 - acc: 0.9428 - val_loss: 0.3312 - val_acc: 0.8850\n",
      "Epoch 13/60\n",
      "113/113 [==============================] - 25s 224ms/step - loss: 0.1343 - acc: 0.9486 - val_loss: 0.3833 - val_acc: 0.8750\n",
      "Epoch 14/60\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.1273 - acc: 0.9492 - val_loss: 0.3655 - val_acc: 0.8875\n",
      "Epoch 15/60\n",
      "113/113 [==============================] - 25s 226ms/step - loss: 0.1188 - acc: 0.9519 - val_loss: 0.3704 - val_acc: 0.8900\n",
      "Epoch 16/60\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.1040 - acc: 0.9625 - val_loss: 0.3996 - val_acc: 0.8975\n",
      "Epoch 17/60\n",
      "113/113 [==============================] - 25s 218ms/step - loss: 0.0994 - acc: 0.9606 - val_loss: 0.4740 - val_acc: 0.8750\n",
      "Epoch 18/60\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.0896 - acc: 0.9683 - val_loss: 0.5462 - val_acc: 0.8525\n",
      "Epoch 19/60\n",
      "113/113 [==============================] - 26s 227ms/step - loss: 0.0882 - acc: 0.9661 - val_loss: 0.3637 - val_acc: 0.8925\n",
      "Epoch 20/60\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.0958 - acc: 0.9642 - val_loss: 0.4862 - val_acc: 0.8650\n",
      "Epoch 21/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.0829 - acc: 0.9697 - val_loss: 0.2968 - val_acc: 0.9075\n",
      "Epoch 22/60\n",
      "113/113 [==============================] - 24s 217ms/step - loss: 0.0746 - acc: 0.9694 - val_loss: 0.3405 - val_acc: 0.9000\n",
      "Epoch 23/60\n",
      "113/113 [==============================] - 24s 214ms/step - loss: 0.0767 - acc: 0.9669 - val_loss: 0.2751 - val_acc: 0.9100\n",
      "Epoch 24/60\n",
      "113/113 [==============================] - 26s 234ms/step - loss: 0.0635 - acc: 0.9758 - val_loss: 0.3005 - val_acc: 0.9150\n",
      "Epoch 25/60\n",
      "113/113 [==============================] - 24s 215ms/step - loss: 0.0595 - acc: 0.9781 - val_loss: 0.4981 - val_acc: 0.8975\n",
      "Epoch 26/60\n",
      "113/113 [==============================] - 24s 215ms/step - loss: 0.0509 - acc: 0.9806 - val_loss: 0.3190 - val_acc: 0.9125\n",
      "Epoch 27/60\n",
      "113/113 [==============================] - 26s 228ms/step - loss: 0.0558 - acc: 0.9794 - val_loss: 0.4813 - val_acc: 0.8875\n",
      "Epoch 28/60\n",
      "113/113 [==============================] - 25s 226ms/step - loss: 0.0561 - acc: 0.9767 - val_loss: 0.3602 - val_acc: 0.9125\n",
      "Epoch 29/60\n",
      "113/113 [==============================] - 25s 225ms/step - loss: 0.0510 - acc: 0.9811 - val_loss: 0.3421 - val_acc: 0.9225\n",
      "Epoch 30/60\n",
      "113/113 [==============================] - 24s 216ms/step - loss: 0.0407 - acc: 0.9889 - val_loss: 0.3803 - val_acc: 0.8800\n",
      "Epoch 31/60\n",
      "113/113 [==============================] - 25s 224ms/step - loss: 0.0505 - acc: 0.9814 - val_loss: 0.3508 - val_acc: 0.9125\n",
      "Epoch 32/60\n",
      "113/113 [==============================] - 24s 213ms/step - loss: 0.0419 - acc: 0.9861 - val_loss: 0.3520 - val_acc: 0.9225\n",
      "Epoch 33/60\n",
      "113/113 [==============================] - 27s 236ms/step - loss: 0.0389 - acc: 0.9833 - val_loss: 0.4778 - val_acc: 0.8725\n",
      "Epoch 34/60\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.0450 - acc: 0.9839 - val_loss: 0.3431 - val_acc: 0.9275\n",
      "Epoch 35/60\n",
      "113/113 [==============================] - 26s 233ms/step - loss: 0.0455 - acc: 0.9833 - val_loss: 0.4171 - val_acc: 0.9075\n",
      "Epoch 36/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.0292 - acc: 0.9903 - val_loss: 0.3716 - val_acc: 0.9200\n",
      "Epoch 37/60\n",
      "113/113 [==============================] - 25s 213ms/step - loss: 0.0298 - acc: 0.9897 - val_loss: 0.3814 - val_acc: 0.9000\n",
      "Epoch 38/60\n",
      "113/113 [==============================] - 25s 217ms/step - loss: 0.0334 - acc: 0.9875 - val_loss: 0.4812 - val_acc: 0.9025\n",
      "Epoch 39/60\n",
      "113/113 [==============================] - 25s 220ms/step - loss: 0.0286 - acc: 0.9881 - val_loss: 0.4400 - val_acc: 0.9025\n",
      "Epoch 40/60\n",
      "113/113 [==============================] - 25s 224ms/step - loss: 0.0234 - acc: 0.9919 - val_loss: 0.3748 - val_acc: 0.9025\n",
      "Epoch 41/60\n",
      "113/113 [==============================] - 25s 219ms/step - loss: 0.0408 - acc: 0.9836 - val_loss: 0.5198 - val_acc: 0.8975\n",
      "Epoch 42/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.0217 - acc: 0.9914 - val_loss: 0.4994 - val_acc: 0.9125\n",
      "Epoch 43/60\n",
      "113/113 [==============================] - 24s 215ms/step - loss: 0.0293 - acc: 0.9897 - val_loss: 0.3239 - val_acc: 0.9325\n",
      "Epoch 44/60\n",
      "113/113 [==============================] - 25s 222ms/step - loss: 0.0256 - acc: 0.9911 - val_loss: 0.3700 - val_acc: 0.9125\n",
      "Epoch 45/60\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.0252 - acc: 0.9933 - val_loss: 0.4420 - val_acc: 0.8975\n",
      "Epoch 46/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.0252 - acc: 0.9911 - val_loss: 0.3493 - val_acc: 0.9350\n",
      "Epoch 47/60\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.0312 - acc: 0.9878 - val_loss: 0.3405 - val_acc: 0.9275\n",
      "Epoch 48/60\n",
      "113/113 [==============================] - 25s 222ms/step - loss: 0.0155 - acc: 0.9953 - val_loss: 0.4637 - val_acc: 0.9150\n",
      "Epoch 49/60\n",
      "113/113 [==============================] - 25s 222ms/step - loss: 0.0175 - acc: 0.9942 - val_loss: 0.4940 - val_acc: 0.9075\n",
      "Epoch 50/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.0123 - acc: 0.9953 - val_loss: 0.4182 - val_acc: 0.9175\n",
      "Epoch 51/60\n",
      "113/113 [==============================] - 25s 220ms/step - loss: 0.0246 - acc: 0.9897 - val_loss: 0.4091 - val_acc: 0.9200\n",
      "Epoch 52/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.0226 - acc: 0.9917 - val_loss: 0.4584 - val_acc: 0.9150\n",
      "Epoch 53/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.0127 - acc: 0.9958 - val_loss: 0.8180 - val_acc: 0.8775\n",
      "Epoch 54/60\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.0192 - acc: 0.9936 - val_loss: 0.6326 - val_acc: 0.8950\n",
      "Epoch 55/60\n",
      "113/113 [==============================] - 25s 224ms/step - loss: 0.0206 - acc: 0.9928 - val_loss: 0.5950 - val_acc: 0.8900\n",
      "Epoch 56/60\n",
      "113/113 [==============================] - 25s 218ms/step - loss: 0.0297 - acc: 0.9911 - val_loss: 0.4699 - val_acc: 0.9050\n",
      "Epoch 57/60\n",
      "113/113 [==============================] - 25s 219ms/step - loss: 0.0189 - acc: 0.9917 - val_loss: 0.4481 - val_acc: 0.9200\n",
      "Epoch 58/60\n",
      "113/113 [==============================] - 26s 227ms/step - loss: 0.0170 - acc: 0.9950 - val_loss: 0.5705 - val_acc: 0.9025\n",
      "Epoch 59/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.0195 - acc: 0.9933 - val_loss: 0.5232 - val_acc: 0.8950\n",
      "Epoch 60/60\n",
      "113/113 [==============================] - 25s 221ms/step - loss: 0.0160 - acc: 0.9958 - val_loss: 0.4813 - val_acc: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e06eadbaf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e0cc0",
   "metadata": {},
   "source": [
    "## 4. Making a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fda3a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.utils as image\n",
    "test_image = image.load_img(\"C:/Users/sesha/Desktop/Projects/CNN - Car or bike/dataset/single_pred/car or bike (3).jpeg\",\n",
    "                            target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'car'\n",
    "else:\n",
    "  prediction = 'bike'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7ab899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67901e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
